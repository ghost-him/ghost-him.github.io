# /// script
# requires-python = ">=3.10"
# ///

"""
Tufted Blog Template æ„å»ºè„šæœ¬

è¿™æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æ„å»ºè„šæœ¬ï¼Œç”¨äºå°† Typst (.typ) æ–‡ä»¶ç¼–è¯‘ä¸º HTML å’Œ PDFï¼Œ
å¹¶å¤åˆ¶é™æ€èµ„æºåˆ°è¾“å‡ºç›®å½•ã€‚

æ”¯æŒå¢é‡ç¼–è¯‘ï¼šåªé‡æ–°ç¼–è¯‘ä¿®æ”¹åçš„æ–‡ä»¶ï¼ŒåŠ å¿«æ„å»ºé€Ÿåº¦ã€‚

ç”¨æ³•:
    uv run build.py build       # å®Œæ•´æ„å»º (HTML + PDF + èµ„æº)
    uv run build.py html        # ä»…æ„å»º HTML æ–‡ä»¶
    uv run build.py pdf         # ä»…æ„å»º PDF æ–‡ä»¶
    uv run build.py assets      # ä»…å¤åˆ¶é™æ€èµ„æº
    uv run build.py clean       # æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶
    uv run build.py preview     # å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ï¼ˆé»˜è®¤ç«¯å£ 8000ï¼‰
    uv run build.py preview -p 3000  # ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£
    uv run build.py --help      # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯

å¢é‡ç¼–è¯‘é€‰é¡¹:
    --force, -f                 # å¼ºåˆ¶å®Œæ•´é‡å»ºï¼Œå¿½ç•¥å¢é‡æ£€æŸ¥

é¢„è§ˆæœåŠ¡å™¨é€‰é¡¹:
    --port, -p PORT             # æŒ‡å®šæœåŠ¡å™¨ç«¯å£å·ï¼ˆé»˜è®¤: 8000ï¼‰

ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ Python è¿è¡Œ:
    python build.py build
    python build.py build --force
    python build.py preview -p 3000
"""

import argparse
import os
import re
import shutil
import subprocess
import sys
import threading
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from html.parser import HTMLParser
from pathlib import Path
from typing import Literal

# ============================================================================
# é…ç½®
# ============================================================================

CONTENT_DIR = Path("content")  # æºæ–‡ä»¶ç›®å½•
SITE_DIR = Path("_site")  # è¾“å‡ºç›®å½•
ASSETS_DIR = Path("assets")  # é™æ€èµ„æºç›®å½•
CONFIG_FILE = Path("config.typ")  # å…¨å±€é…ç½®æ–‡ä»¶


@dataclass
class BuildStats:
    """æ„å»ºç»Ÿè®¡ä¿¡æ¯"""

    success: int = 0
    skipped: int = 0
    failed: int = 0

    def format_summary(self) -> str:
        """æ ¼å¼åŒ–ç»Ÿè®¡æ‘˜è¦"""
        parts = []
        if self.success > 0:
            parts.append(f"ç¼–è¯‘: {self.success}")
        if self.skipped > 0:
            parts.append(f"è·³è¿‡: {self.skipped}")
        if self.failed > 0:
            parts.append(f"å¤±è´¥: {self.failed}")
        return ", ".join(parts) if parts else "æ— æ–‡ä»¶éœ€è¦å¤„ç†"

    @property
    def has_failures(self) -> bool:
        """æ˜¯å¦å­˜åœ¨å¤±è´¥"""
        return self.failed > 0


class HTMLMetadataParser(HTMLParser):
    """
    ä» HTML æ–‡ä»¶ä¸­æå–å…ƒæ•°æ®çš„è§£æå™¨ã€‚

    è§£æä»¥ä¸‹å…ƒæ•°æ®ï¼š
    - lang: ä» <html lang="..."> å±æ€§è·å–
    - title: ä» <title> æ ‡ç­¾è·å–
    - description: ä» <meta name="description" content="..."> è·å–
    - link: ä» <link rel="canonical" href="..."> è·å–
    - date: ä» <meta name="date" content="..."> è·å–
    """

    def __init__(self):
        super().__init__()
        self.metadata = {"title": ""}
        self._in_title = False

    def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]):
        attrs_dict = {k: v for k, v in attrs if v}

        match tag:
            case "html":
                self.metadata["lang"] = attrs_dict.get("lang", "")
            case "title":
                self._in_title = True
            case "meta":
                name = attrs_dict.get("name", "")
                if name in {"description", "date"}:
                    self.metadata[name] = attrs_dict.get("content", "")
            case "link":
                if attrs_dict.get("rel") == "canonical":
                    self.metadata["link"] = attrs_dict.get("href", "")

    def handle_endtag(self, tag: str):
        if tag == "title":
            self._in_title = False

    def handle_data(self, data: str):
        if self._in_title:
            self.metadata["title"] += data


# ============================================================================
# å¢é‡ç¼–è¯‘è¾…åŠ©å‡½æ•°
# ============================================================================


def get_file_mtime(path: Path) -> float:
    """
    è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æˆ³ã€‚

    å‚æ•°:
        path: æ–‡ä»¶è·¯å¾„

    è¿”å›:
        float: ä¿®æ”¹æ—¶é—´æˆ³ï¼Œæ–‡ä»¶ä¸å­˜åœ¨è¿”å› 0
    """
    try:
        return path.stat().st_mtime
    except (OSError, FileNotFoundError):
        return 0.0


def is_dep_file(path: Path) -> bool:
    """
    åˆ¤æ–­ä¸€ä¸ªæ–‡ä»¶æ˜¯å¦è¢«è¿½è¸ªä¸ºä¾èµ–ï¼‰ã€‚

    content/ ä¸‹çš„æ™®é€šé¡µé¢æ–‡ä»¶ä¸è¢«è§†ä¸ºæ¨¡æ¿æ–‡ä»¶ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç‹¬ç«‹çš„é¡µé¢ï¼Œ
    ä¸åº”è¯¥ç›¸äº’ä¾èµ–ã€‚

    å‚æ•°:
        path: æ–‡ä»¶è·¯å¾„

    è¿”å›:
        bool: æ˜¯å¦æ˜¯ä¾èµ–æ–‡ä»¶
    """
    try:
        resolved_path = path.resolve()
        project_root = Path(__file__).parent.resolve()
        content_dir = (project_root / CONTENT_DIR).resolve()

        # config.typ æ˜¯ä¾èµ–æ–‡ä»¶
        if resolved_path == (project_root / CONFIG_FILE).resolve():
            return True

        # æ£€æŸ¥æ˜¯å¦åœ¨ content/ ç›®å½•ä¸‹
        try:
            relative_to_content = resolved_path.relative_to(content_dir)
            # content/_* ç›®å½•ä¸‹çš„æ–‡ä»¶è§†ä¸ºä¾èµ–æ–‡ä»¶
            parts = relative_to_content.parts
            if len(parts) > 0 and parts[0].startswith("_"):
                return True
            # content/ ä¸‹çš„å…¶ä»–æ–‡ä»¶ä¸æ˜¯ä¾èµ–æ–‡ä»¶
            return False
        except ValueError:
            # ä¸åœ¨ content/ ç›®å½•ä¸‹ï¼Œè§†ä¸ºä¾èµ–æ–‡ä»¶ï¼ˆå¦‚ config.typï¼‰
            return True

    except Exception:
        return True


def find_typ_dependencies(typ_file: Path) -> set[Path]:
    """
    è§£æ .typ æ–‡ä»¶ä¸­çš„ä¾èµ–ï¼ˆé€šè¿‡ #import å’Œ #include å¯¼å…¥çš„æ–‡ä»¶ï¼‰ã€‚

    åªè¿½è¸ª .typ æ–‡ä»¶çš„ä¾èµ–ï¼Œå¿½ç•¥ content/ ä¸‹çš„æ™®é€šé¡µé¢æ–‡ä»¶ã€‚
    å…¶ä»–èµ„æºæ–‡ä»¶ï¼ˆå¦‚ .md, .bib, å›¾ç‰‡ç­‰ï¼‰é€šè¿‡ copy_content_assets å¤„ç†ã€‚

    å‚æ•°:
        typ_file: .typ æ–‡ä»¶è·¯å¾„

    è¿”å›:
        set[Path]: ä¾èµ–çš„ .typ æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    dependencies: set[Path] = set()

    try:
        content = typ_file.read_text(encoding="utf-8")
    except Exception:
        return dependencies

    # è·å–æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œç”¨äºè§£æç›¸å¯¹è·¯å¾„
    base_dir = typ_file.parent

    patterns = [
        r'#import\s+"([^"]+)"',
        r"#import\s+'([^']+)'",
        r'#include\s+"([^"]+)"',
        r"#include\s+'([^']+)'",
    ]

    for pattern in patterns:
        for match in re.finditer(pattern, content):
            dep_path_str = match.group(1)

            # è·³è¿‡åŒ…å¯¼å…¥ï¼ˆå¦‚ @preview/xxxï¼‰
            if dep_path_str.startswith("@"):
                continue

            # è§£æç›¸å¯¹è·¯å¾„
            if dep_path_str.startswith("/"):
                # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
                dep_path = Path(dep_path_str.lstrip("/"))
            else:
                # ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„
                dep_path = base_dir / dep_path_str

            # è§„èŒƒåŒ–è·¯å¾„ï¼Œåªè¿½è¸ª .typ æ–‡ä»¶
            try:
                dep_path = dep_path.resolve()
                if dep_path.exists() and dep_path.suffix == ".typ" and is_dep_file(dep_path):
                    dependencies.add(dep_path)
            except Exception:
                pass

    return dependencies


def get_all_dependencies(typ_file: Path, visited: set[Path] | None = None) -> set[Path]:
    """
    é€’å½’è·å– .typ æ–‡ä»¶çš„æ‰€æœ‰ä¾èµ–ï¼ˆåŒ…æ‹¬ä¼ é€’ä¾èµ–ï¼‰ã€‚

    å‚æ•°:
        typ_file: .typ æ–‡ä»¶è·¯å¾„
        visited: å·²è®¿é—®çš„æ–‡ä»¶é›†åˆï¼ˆç”¨äºé¿å…å¾ªç¯ä¾èµ–ï¼‰

    è¿”å›:
        set[Path]: æ‰€æœ‰ä¾èµ–æ–‡ä»¶è·¯å¾„é›†åˆ
    """
    if visited is None:
        visited = set()

    # é¿å…å¾ªç¯ä¾èµ–
    abs_path = typ_file.resolve()
    if abs_path in visited:
        return set()
    visited.add(abs_path)

    all_deps: set[Path] = set()
    direct_deps = find_typ_dependencies(typ_file)

    for dep in direct_deps:
        all_deps.add(dep)
        # åªå¯¹ .typ æ–‡ä»¶é€’å½’æŸ¥æ‰¾ä¾èµ–
        if dep.suffix == ".typ":
            all_deps.update(get_all_dependencies(dep, visited))

    return all_deps


def needs_rebuild(source: Path, target: Path, extra_deps: list[Path] | None = None) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ„å»ºã€‚

    å½“ä»¥ä¸‹ä»»ä¸€æ¡ä»¶æ»¡è¶³æ—¶éœ€è¦é‡å»ºï¼š
    1. ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨
    2. æºæ–‡ä»¶æ¯”ç›®æ ‡æ–‡ä»¶æ–°
    3. ä»»ä½•é¢å¤–ä¾èµ–æ–‡ä»¶æ¯”ç›®æ ‡æ–‡ä»¶æ–°
    4. æºæ–‡ä»¶çš„ä»»ä½•å¯¼å…¥ä¾èµ–æ¯”ç›®æ ‡æ–‡ä»¶æ–°
    5. æºæ–‡ä»¶åŒç›®å½•ä¸‹çš„ä»»ä½•é .typ æ–‡ä»¶æ¯”ç›®æ ‡æ–‡ä»¶æ–°ï¼ˆå¦‚ .md, .bib, å›¾ç‰‡ç­‰ï¼‰

    å‚æ•°:
        source: æºæ–‡ä»¶è·¯å¾„
        target: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        extra_deps: é¢å¤–çš„ä¾èµ–æ–‡ä»¶åˆ—è¡¨ï¼ˆå¦‚ config.typï¼‰

    è¿”å›:
        bool: æ˜¯å¦éœ€è¦é‡æ–°æ„å»º
    """
    # ç›®æ ‡ä¸å­˜åœ¨ï¼Œéœ€è¦æ„å»º
    if not target.exists():
        return True

    target_mtime = get_file_mtime(target)

    # æºæ–‡ä»¶æ›´æ–°äº†
    if get_file_mtime(source) > target_mtime:
        return True

    # æ£€æŸ¥é¢å¤–ä¾èµ–
    if extra_deps:
        for dep in extra_deps:
            if dep.exists() and get_file_mtime(dep) > target_mtime:
                return True

    # æ£€æŸ¥æºæ–‡ä»¶çš„å¯¼å…¥ä¾èµ–
    for dep in get_all_dependencies(source):
        if get_file_mtime(dep) > target_mtime:
            return True

    # æ£€æŸ¥æºæ–‡ä»¶åŒç›®å½•ä¸‹çš„é .typ èµ„æºæ–‡ä»¶ï¼ˆå¦‚ .md, .bib, å›¾ç‰‡ç­‰ï¼‰
    # åªæ£€æŸ¥åŒä¸€ç›®å½•ï¼Œä¸é€’å½’å­ç›®å½•ï¼Œé¿å…è¿‡åº¦é‡ç¼–è¯‘
    source_dir = source.parent
    for item in source_dir.iterdir():
        if item.is_file() and item.suffix != ".typ":
            if get_file_mtime(item) > target_mtime:
                return True

    return False


def find_common_dependencies() -> list[Path]:
    """
    æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶çš„å…¬å…±ä¾èµ–ï¼ˆå¦‚ config.typï¼‰ã€‚

    è¿”å›:
        list[Path]: å…¬å…±ä¾èµ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    common_deps = []

    # config.typ æ˜¯å…¨å±€é…ç½®ï¼Œä¿®æ”¹åæ‰€æœ‰é¡µé¢éƒ½éœ€è¦é‡å»º
    if CONFIG_FILE.exists():
        common_deps.append(CONFIG_FILE)

    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å…¶ä»–å…¬å…±ä¾èµ–
    # ä¾‹å¦‚ï¼šæŸ¥æ‰¾ content/_* ç›®å½•ä¸‹çš„æ¨¡æ¿æ–‡ä»¶
    if CONTENT_DIR.exists():
        for item in CONTENT_DIR.iterdir():
            if item.is_dir() and item.name.startswith("_"):
                for typ_file in item.rglob("*.typ"):
                    common_deps.append(typ_file)

    return common_deps


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================


def find_typ_files() -> list[Path]:
    """
    æŸ¥æ‰¾ content/ ç›®å½•ä¸‹æ‰€æœ‰ .typ æ–‡ä»¶ï¼Œæ’é™¤è·¯å¾„ä¸­åŒ…å«ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„ç›®å½•çš„æ–‡ä»¶ã€‚

    è¿”å›:
        list[Path]: .typ æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    typ_files = []
    for typ_file in CONTENT_DIR.rglob("*.typ"):
        # æ£€æŸ¥è·¯å¾„ä¸­æ˜¯å¦æœ‰ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„ç›®å½•
        parts = typ_file.relative_to(CONTENT_DIR).parts
        if not any(part.startswith("_") for part in parts):
            typ_files.append(typ_file)
    return typ_files


def get_file_output_path(typ_file: Path, type: Literal["pdf", "html"]) -> Path:
    """
    è·å– .typ æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„ã€‚

    å‚æ•°:
        typ_file: .typ æ–‡ä»¶è·¯å¾„ (ç›¸å¯¹äº content/)

    è¿”å›:
        Path: æ–‡ä»¶è¾“å‡ºè·¯å¾„ (åœ¨ _site/ ç›®å½•ä¸‹)
    """
    relative_path = typ_file.relative_to(CONTENT_DIR)
    return SITE_DIR / relative_path.with_suffix(f".{type}")


def run_typst_command(args: list[str]) -> bool:
    """
    è¿è¡Œ typst å‘½ä»¤ã€‚

    å‚æ•°:
        args: typst å‘½ä»¤å‚æ•°åˆ—è¡¨

    è¿”å›:
        bool: å‘½ä»¤æ˜¯å¦æˆåŠŸæ‰§è¡Œ
    """
    try:
        result = subprocess.run(["typst"] + args, capture_output=True, text=True, encoding="utf-8")
        if result.returncode != 0:
            print(f"  âŒ Typst é”™è¯¯: {result.stderr.strip()}")
            return False
        return True
    except FileNotFoundError:
        print("  âŒ é”™è¯¯: æœªæ‰¾åˆ° typst å‘½ä»¤ã€‚è¯·ç¡®ä¿å·²å®‰è£… Typst å¹¶æ·»åŠ åˆ° PATH ç¯å¢ƒå˜é‡ä¸­ã€‚")
        print("  ğŸ“ å®‰è£…è¯´æ˜: https://typst.app/open-source/#download")
        return False
    except Exception as e:
        print(f"  âŒ æ‰§è¡Œ typst å‘½ä»¤æ—¶å‡ºé”™: {e}")
        return False


# ============================================================================
# æ„å»ºå‘½ä»¤
# ============================================================================


def _compile_files(
    files: list[Path],
    force: bool,
    common_deps: list[Path],
    get_output_path_func,
    build_args_func,
) -> BuildStats:
    """
    é€šç”¨æ–‡ä»¶ç¼–è¯‘å‡½æ•°ï¼Œå‡å°‘é‡å¤ä»£ç ã€‚

    å‚æ•°:
        files: è¦ç¼–è¯‘çš„æ–‡ä»¶åˆ—è¡¨
        force: æ˜¯å¦å¼ºåˆ¶é‡å»º
        common_deps: å…¬å…±ä¾èµ–åˆ—è¡¨
        get_output_path_func: è·å–è¾“å‡ºè·¯å¾„çš„å‡½æ•°
        build_args_func: æ„å»ºç¼–è¯‘å‚æ•°çš„å‡½æ•°

    è¿”å›:
        BuildStats: æ„å»ºç»Ÿè®¡ä¿¡æ¯
    """
    stats = BuildStats()

    for typ_file in files:
        output_path = get_output_path_func(typ_file)

        # å¢é‡ç¼–è¯‘æ£€æŸ¥
        if not force and not needs_rebuild(typ_file, output_path, common_deps):
            stats.skipped += 1
            continue

        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ„å»ºç¼–è¯‘å‚æ•°
        args = build_args_func(typ_file, output_path)

        if run_typst_command(args):
            stats.success += 1
        else:
            print(f"  âŒ {typ_file} ç¼–è¯‘å¤±è´¥")
            stats.failed += 1

    return stats


def build_html(force: bool = False) -> bool:
    """
    ç¼–è¯‘æ‰€æœ‰ .typ æ–‡ä»¶ä¸º HTMLï¼ˆæ–‡ä»¶åä¸­åŒ…å« PDF çš„é™¤å¤–ï¼‰ã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»¶
    """
    SITE_DIR.mkdir(parents=True, exist_ok=True)

    typ_files = find_typ_files()

    # æ’é™¤æ ‡è®°ä¸º PDF çš„æ–‡ä»¶
    html_files = [f for f in typ_files if "pdf" not in f.stem.lower()]

    if not html_files:
        print("  âš ï¸ æœªæ‰¾åˆ°ä»»ä½• HTML æ–‡ä»¶ã€‚")
        return True

    print("æ­£åœ¨æ„å»º HTML æ–‡ä»¶...")

    # è·å–å…¬å…±ä¾èµ–
    common_deps = find_common_dependencies()

    def build_html_args(typ_file: Path, output_path: Path) -> list[str]:
        """æ„å»º HTML ç¼–è¯‘å‚æ•°"""
        try:
            rel_path = typ_file.relative_to(CONTENT_DIR)

            if rel_path.name == "index.typ":
                # index.typ uses the parent directory name as the path
                # content/Blog/index.typ -> "Blog"
                # content/index.typ -> "" (Homepage)
                page_path = rel_path.parent.as_posix()
                if page_path == ".":
                    page_path = ""
            else:
                # Common files use the filename as the path
                # content/about.typ -> "about"
                page_path = rel_path.with_suffix("").as_posix()
        except ValueError:
            page_path = ""

        return [
            "compile",
            "--root",
            ".",
            "--font-path",
            str(ASSETS_DIR),
            "--features",
            "html",
            "--format",
            "html",
            "--input",
            f"page-path={page_path}",
            str(typ_file),
            str(output_path),
        ]

    stats = _compile_files(
        html_files,
        force,
        common_deps,
        lambda typ_file: get_file_output_path(typ_file, "html"),
        build_html_args,
    )

    print(f"âœ… HTML æ„å»ºå®Œæˆã€‚{stats.format_summary()}")
    return not stats.has_failures


def build_pdf(force: bool = False) -> bool:
    """
    ç¼–è¯‘æ–‡ä»¶ååŒ…å« "PDF" çš„ .typ æ–‡ä»¶ä¸º PDFã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»¶
    """
    SITE_DIR.mkdir(parents=True, exist_ok=True)

    typ_files = find_typ_files()
    pdf_files = [f for f in typ_files if "pdf" in f.stem.lower()]

    if not pdf_files:
        return True

    print("æ­£åœ¨æ„å»º PDF æ–‡ä»¶...")

    # è·å–å…¬å…±ä¾èµ–
    common_deps = find_common_dependencies()

    def build_pdf_args(typ_file: Path, output_path: Path) -> list[str]:
        """æ„å»º PDF ç¼–è¯‘å‚æ•°"""
        return [
            "compile",
            "--root",
            ".",
            "--font-path",
            str(ASSETS_DIR),
            str(typ_file),
            str(output_path),
        ]

    stats = _compile_files(
        pdf_files,
        force,
        common_deps,
        lambda typ_file: get_file_output_path(typ_file, "pdf"),
        build_pdf_args,
    )

    print(f"âœ… PDF æ„å»ºå®Œæˆã€‚{stats.format_summary()}")
    return not stats.has_failures


def copy_assets() -> bool:
    """
    å¤åˆ¶é™æ€èµ„æºåˆ°è¾“å‡ºç›®å½•ã€‚
    """
    if not ASSETS_DIR.exists():
        print(f"  âš  é™æ€èµ„æºç›®å½• {ASSETS_DIR} ä¸å­˜åœ¨ã€‚")
        return True

    SITE_DIR.mkdir(parents=True, exist_ok=True)
    target_dir = SITE_DIR / "assets"

    try:
        if target_dir.exists():
            shutil.rmtree(target_dir)
        shutil.copytree(ASSETS_DIR, target_dir)
        return True
    except Exception as e:
        print(f"  âŒ å¤åˆ¶é™æ€èµ„æºå¤±è´¥: {e}")
        return False


def copy_content_assets(force: bool = False) -> bool:
    """
    å¤åˆ¶ content ç›®å½•ä¸‹çš„é .typ æ–‡ä»¶ï¼ˆå¦‚å›¾ç‰‡ï¼‰åˆ°è¾“å‡ºç›®å½•ã€‚
    æ”¯æŒå¢é‡å¤åˆ¶ï¼šåªå¤åˆ¶ä¿®æ”¹è¿‡çš„æ–‡ä»¶ã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶å¤åˆ¶æ‰€æœ‰æ–‡ä»¶
    """
    SITE_DIR.mkdir(parents=True, exist_ok=True)

    if not CONTENT_DIR.exists():
        print(f"  âš  å†…å®¹ç›®å½• {CONTENT_DIR} ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
        return True

    try:
        copy_count = 0
        skip_count = 0

        for item in CONTENT_DIR.rglob("*"):
            # è·³è¿‡ç›®å½•å’Œ .typ æ–‡ä»¶
            if item.is_dir() or item.suffix == ".typ":
                continue

            # è·³è¿‡ä»¥ä¸‹åˆ’çº¿å¼€å¤´çš„è·¯å¾„
            relative_path = item.relative_to(CONTENT_DIR)
            if any(part.startswith("_") for part in relative_path.parts):
                continue

            # è®¡ç®—ç›®æ ‡è·¯å¾„
            target_path = SITE_DIR / relative_path

            # å¢é‡å¤åˆ¶æ£€æŸ¥
            if not force and target_path.exists():
                if get_file_mtime(item) <= get_file_mtime(target_path):
                    skip_count += 1
                    continue

            # åˆ›å»ºç›®æ ‡ç›®å½•
            target_path.parent.mkdir(parents=True, exist_ok=True)

            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(item, target_path)
            copy_count += 1

        return True
    except Exception as e:
        print(f"  âŒ å¤åˆ¶å†…å®¹èµ„æºæ–‡ä»¶å¤±è´¥: {e}")
        return False


def clean() -> bool:
    """
    æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶ã€‚
    """
    print("æ­£åœ¨æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶...")

    if not SITE_DIR.exists():
        print(f"  è¾“å‡ºç›®å½• {SITE_DIR} ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚")
        return True

    try:
        # åˆ é™¤ _site ç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹
        for item in SITE_DIR.iterdir():
            if item.is_dir():
                shutil.rmtree(item)
            else:
                item.unlink()

        print(f"  âœ… å·²æ¸…ç† {SITE_DIR}/ ç›®å½•ã€‚")
        return True
    except Exception as e:
        print(f"  âŒ æ¸…ç†å¤±è´¥: {e}")
        return False


def preview(port: int = 8000, open_browser_flag: bool = True) -> bool:
    """
    å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ã€‚

    é¦–å…ˆå°è¯•ä½¿ç”¨ uvx livereloadï¼ˆæ”¯æŒå®æ—¶åˆ·æ–°ï¼‰ï¼Œ
    å¦‚æœå¤±è´¥åˆ™å›é€€åˆ° Python å†…ç½®çš„ http.serverã€‚

    å‚æ•°:
        port: æœåŠ¡å™¨ç«¯å£å·ï¼Œé»˜è®¤ä¸º 8000
        open_browser_flag: æ˜¯å¦è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼Œé»˜è®¤ä¸º True
    """
    import webbrowser

    if not SITE_DIR.exists():
        print(f"  âš  è¾“å‡ºç›®å½• {SITE_DIR} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build å‘½ä»¤ã€‚")
        return False

    print("æ­£åœ¨å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ï¼ˆæŒ‰ Ctrl+C åœæ­¢ï¼‰...")
    print()

    if open_browser_flag:

        def open_browser():
            time.sleep(1.5)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
            url = f"http://localhost:{port}"
            print(f"  ğŸš€ æ­£åœ¨æ‰“å¼€æµè§ˆå™¨: {url}")
            webbrowser.open(url)

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰“å¼€æµè§ˆå™¨
        threading.Thread(target=open_browser, daemon=True).start()

    # é¦–å…ˆå°è¯• uvx livereload
    try:
        result = subprocess.run(
            ["uvx", "livereload", str(SITE_DIR), "-p", str(port)],
            check=False,
        )
        return result.returncode == 0
    except FileNotFoundError:
        print("  æœªæ‰¾åˆ° uvï¼Œå°è¯• Python http.server...")
    except KeyboardInterrupt:
        print("\næœåŠ¡å™¨å·²åœæ­¢ã€‚")
        return True

    # å›é€€åˆ° Python http.server
    try:
        print("ä½¿ç”¨ Python å†…ç½® http.server...")
        result = subprocess.run(
            [sys.executable, "-m", "http.server", str(port), "--directory", str(SITE_DIR)],
            check=False,
        )
        return result.returncode == 0
    except KeyboardInterrupt:
        print("\næœåŠ¡å™¨å·²åœæ­¢ã€‚")
        return True
    except Exception as e:
        print(f"  âŒ å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {e}")
        return False


def parse_html_metadata(html_path: Path) -> dict[str, str]:
    """
    è§£æ HTML æ–‡ä»¶å¹¶è¿”å›å…ƒæ•°æ®è§£æå™¨å®ä¾‹ã€‚

    å‚æ•°:
        html_path (Path): HTML æ–‡ä»¶è·¯å¾„

    è¿”å›:
        HTMLMetadataParser: åŒ…å«è§£æç»“æœçš„è§£æå™¨å®ä¾‹
    """
    parser = HTMLMetadataParser()
    parser.feed(html_path.read_text(encoding="utf-8"))
    return parser.metadata


def get_site_url() -> str | None:
    """
    ä»ç”Ÿæˆçš„é¦–é¡µ HTML æ–‡ä»¶ä¸­è§£æç«™ç‚¹ URLã€‚

    åŠŸèƒ½:
        ä» _site/index.html çš„ <link rel="canonical" href="..."> æå– site-urlã€‚

    è¿”å›:
        str: ç«™ç‚¹çš„æ ¹ URLï¼ˆå¦‚ "https://example.com"ï¼‰ï¼Œæœ«å°¾ä¸å¸¦æ–œæ ã€‚
            å¦‚æœæœªé…ç½®æˆ–è§£æå¤±è´¥åˆ™è¿”å› Noneã€‚
    """
    index_html = SITE_DIR / "index.html"
    parser = parse_html_metadata(index_html)

    if parser.get("link"):
        return parser["link"].rstrip("/")

    return None


def get_feed_dirs() -> set[str]:
    """
    ä» config.typ é…ç½®æ–‡ä»¶ä¸­è§£æ RSS Feed è®¢é˜…æºçš„é…ç½®ä¿¡æ¯ã€‚

    åŠŸèƒ½:
        è§£æ config.typ ä¸­çš„ feed é…ç½®å—ï¼Œæå–ç›®å½•åˆ—è¡¨ã€‚

    è¿”å›:
        set[str]: è¦åŒ…å«çš„æ–‡ç« ç›®å½•åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºç©ºé›†åˆ
    """
    if not CONFIG_FILE.exists():
        return set()

    try:
        content = CONFIG_FILE.read_text(encoding="utf-8")

        # ç§»é™¤æ³¨é‡Š
        content = re.sub(r"//.*", "", content)
        content = re.sub(r"/\*[\s\S]*?\*/", "", content)

        match = re.search(r"feed-dir\s*:\s*\((.*?)\)", content, re.DOTALL)
        if match:
            return set(
                c.strip("/") for c in re.findall(r'"([^"]*)"', match.group(1)) if c and c.strip("/")
            )
    except Exception as e:
        print(f"âš ï¸ è§£æ feed-dir å¤±è´¥: {e}")

    return set()


def extract_post_metadata(index_html: Path) -> tuple[str, str, str, datetime | None]:
    """
    ä»ç”Ÿæˆçš„ HTML æ–‡ä»¶ä¸­æå–æ–‡ç« çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚

    åŠŸèƒ½:
        æå–æ–‡ç« å…ƒæ•°æ®ï¼š
        1. æ ‡é¢˜ (title): ä» <title> æ ‡ç­¾æå–
        2. æè¿° (description): ä» <meta name="description"> æå–
        3. é“¾æ¥ (link): ä» <link rel="canonical" href="..."> æå–
        4. æ—¥æœŸ (date): ä¾æ¬¡å°è¯•ä»ä»¥ä¸‹æ¥æºè·å–ï¼š
            - HTML ä¸­çš„ <meta name="date" content="...">
            - æ–‡ä»¶å¤¹åä¸­çš„ YYYY-MM-DD æ ¼å¼æ—¥æœŸ

    å‚æ•°:
        index_html (Path): æ–‡ç« çš„ index.html æ–‡ä»¶è·¯å¾„

    è¿”å›:
        tuple[str, str, str, datetime | None]: åŒ…å«å››ä¸ªå…ƒç´ çš„å…ƒç»„ï¼š
            - str: æ–‡ç« æ ‡é¢˜
            - str: æ–‡ç« æè¿°ï¼ˆå¯èƒ½ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
            - str: æ–‡ç« é“¾æ¥ï¼ˆå®Œæ•´ URLï¼‰
            - datetime | None: æ–‡ç« æ—¥æœŸï¼ˆå¸¦ UTC æ—¶åŒºï¼‰ï¼Œæ— æ³•è·å–æ—¶ä¸º None
    """
    parser = parse_html_metadata(index_html)

    title = parser["title"].strip()
    description = parser.get("description", "").strip()
    link = parser.get("link", "")
    date_obj = None

    # å°è¯•ä» <meta name="date"> è§£ææ—¥æœŸ
    if parser.get("date"):
        try:
            date_obj = datetime.strptime(parser["date"].split("T")[0], "%Y-%m-%d")
            date_obj = date_obj.replace(tzinfo=timezone.utc)
        except Exception:
            pass

    # å¦‚æœæ²¡æ‰¾åˆ°æ—¥æœŸï¼Œå°è¯•ä»æ–‡ä»¶å¤¹åæå– (YYYY-MM-DD)
    if not date_obj:
        date_match = re.search(r"(\d{4}-\d{2}-\d{2})", index_html.parent.name)
        if date_match:
            try:
                date_obj = datetime.strptime(date_match.group(1), "%Y-%m-%d")
                date_obj = date_obj.replace(tzinfo=timezone.utc)
            except ValueError:
                pass

    return title, description, link, date_obj


def collect_posts(dirs: set[str], site_url: str) -> list[dict]:
    """
    ä»æŒ‡å®šçš„ç›®å½•ä¸­æ”¶é›†æ‰€æœ‰æ–‡ç« çš„å…ƒæ•°æ®ã€‚

    åŠŸèƒ½:
        éå† _site ç›®å½•ä¸‹æŒ‡å®šç›®å½•ä¸­çš„æ‰€æœ‰å­ç›®å½•ï¼Œæå–æ¯ä¸ªæ–‡ç« çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚
        åªå¤„ç†ç›®å½•ï¼ˆæ¯ä¸ªç›®å½•ä»£è¡¨ä¸€ç¯‡æ–‡ç« ï¼‰ï¼Œè·³è¿‡æ™®é€šæ–‡ä»¶ã€‚
        å¦‚æœæ— æ³•ç¡®å®šæ–‡ç« æ—¥æœŸï¼Œåˆ™è·³è¿‡è¯¥æ–‡ç« å¹¶è¾“å‡ºè­¦å‘Šã€‚

    å‚æ•°:
        dirs (set[str]): è¦æ‰«æçš„ç›®å½•åç§°é›†åˆï¼ˆå¦‚ {"Blog", "Docs"}ï¼‰
        site_url (str): ç«™ç‚¹çš„æ ¹ URLï¼ˆå¦‚ "https://example.com"ï¼‰

    è¿”å›:
        list[dict]: æ–‡ç« æ•°æ®å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«ä»¥ä¸‹é”®ï¼š
            - title (str): æ–‡ç« æ ‡é¢˜
            - description (str): æ–‡ç« æè¿°
            - dir (str): æ–‡ç« æ‰€å±åˆ†ç±»ï¼ˆå³ç›®å½•åï¼‰
            - link (str): æ–‡ç« çš„å®Œæ•´ URL
            - date (datetime): æ–‡ç« æ—¥æœŸå¯¹è±¡ï¼ˆå¸¦æ—¶åŒºï¼‰
    """
    posts = []

    for d in dirs:
        dir_path = SITE_DIR / d

        for item in dir_path.iterdir():
            if not item.is_dir():
                continue

            index_html = item / "index.html"
            if not index_html.exists():
                continue

            title, description, link, date_obj = extract_post_metadata(index_html)

            if not date_obj:
                print(f"âš ï¸ æ— æ³•ç¡®å®šæ–‡ç«  '{item.name}' çš„æ—¥æœŸï¼Œå·²è·³è¿‡ã€‚")
                continue

            posts.append(
                {
                    "title": title,
                    "description": description,
                    "dir": d,
                    "link": link,
                    "date": date_obj,
                }
            )

    return posts


def build_rss_xml(posts: list[dict], config: dict) -> str:
    """
    æ„å»ºç¬¦åˆ RSS 2.0 è§„èŒƒçš„ XML å†…å®¹å­—ç¬¦ä¸²ã€‚

    åŠŸèƒ½:
        ä½¿ç”¨ Python æ ‡å‡†åº“ xml.etree.ElementTree æ ¹æ®æ–‡ç« æ•°æ®å’Œç«™ç‚¹é…ç½®ç”Ÿæˆå®Œæ•´çš„ RSS Feed XMLã€‚
        æ”¯æŒæ¡ä»¶è¾“å‡º description æ ‡ç­¾ï¼ˆä»…åœ¨æœ‰æè¿°æ—¶è¾“å‡ºï¼‰ã€‚

    å‚æ•°:
        posts (list[dict]): æ–‡ç« æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åº”åŒ…å«:
            - title: æ ‡é¢˜
            - description: æè¿°ï¼ˆå¯é€‰ï¼‰
            - link: æ–‡ç« é“¾æ¥
            - date: datetime å¯¹è±¡
            - dir: åˆ†ç±»åç§° (å³è·¯å¾„å)
        config (dict): ç«™ç‚¹é…ç½®å­—å…¸ï¼Œåº”åŒ…å«:
            - site_url: ç«™ç‚¹æ ¹ URL
            - site_title: ç«™ç‚¹æ ‡é¢˜
            - site_description: ç«™ç‚¹æè¿°
            - lang: è¯­è¨€ä»£ç ï¼ˆå¦‚ "zh", "en"ï¼‰

    è¿”å›:
        str: å®Œæ•´çš„ RSS 2.0 XML å­—ç¬¦ä¸²ï¼ŒåŒ…å« XML å£°æ˜å’Œæ‰€æœ‰å¿…è¦çš„å‘½åç©ºé—´ã€‚
    """
    import xml.etree.ElementTree as ET
    from email.utils import format_datetime

    # æ³¨å†Œ atom å‘½åç©ºé—´å‰ç¼€
    ATOM_NS = "http://www.w3.org/2005/Atom"
    ET.register_namespace("atom", ATOM_NS)

    # åˆ›å»º RSS æ ¹å…ƒç´ ï¼ˆå‘½åç©ºé—´å£°æ˜ç”± register_namespace è‡ªåŠ¨å¤„ç†ï¼‰
    rss = ET.Element("rss", version="2.0")

    # Channel å…ƒæ•°æ®
    channel = ET.SubElement(rss, "channel")
    ET.SubElement(channel, "title").text = config["site_title"]
    ET.SubElement(channel, "link").text = config["site_url"]
    ET.SubElement(channel, "description").text = config["site_description"]
    ET.SubElement(channel, "language").text = config["lang"]
    ET.SubElement(channel, "lastBuildDate").text = format_datetime(datetime.now(timezone.utc))

    # æ·»åŠ  atom:link è‡ªé“¾æ¥
    atom_link = ET.SubElement(channel, f"{{{ATOM_NS}}}link")
    atom_link.set("href", f"{config['site_url']}/feed.xml")
    atom_link.set("rel", "self")
    atom_link.set("type", "application/rss+xml")

    # æ·»åŠ æ–‡ç« æ¡ç›®
    for post in posts:
        item = ET.SubElement(channel, "item")

        ET.SubElement(item, "title").text = post["title"]
        ET.SubElement(item, "link").text = post["link"]
        ET.SubElement(item, "guid", isPermaLink="true").text = post["link"]
        ET.SubElement(item, "pubDate").text = format_datetime(post["date"])
        ET.SubElement(item, "category").text = post["dir"]

        # ä»…åœ¨æœ‰æè¿°æ—¶æ·»åŠ 
        if des := post["description"]:
            ET.SubElement(item, "description").text = des

    # ç”Ÿæˆ XML å­—ç¬¦ä¸²
    ET.indent(rss, space="  ")
    xml_str = ET.tostring(rss, encoding="unicode", xml_declaration=False)

    return f'<?xml version="1.0" encoding="UTF-8"?>\n{xml_str}'


def generate_rss(site_url: str) -> bool:
    """
    ç”Ÿæˆç½‘ç«™çš„ RSS è®¢é˜…æºæ–‡ä»¶ã€‚

    åŠŸèƒ½:
        å®Œæ•´çš„ RSS Feed ç”Ÿæˆæµç¨‹ï¼š
        1. ä» config.typ è¯»å–ç›®æ ‡ç›®å½•ï¼ˆåˆ†ç±»ï¼‰
        2. æ”¶é›†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ç« å…ƒæ•°æ®
        3. æŒ‰æ—¥æœŸæ’åº
        4. æ„å»º RSS XML å¹¶å†™å…¥æ–‡ä»¶

    è¿”å›:
        bool: ç”Ÿæˆæ˜¯å¦æˆåŠŸã€‚åœ¨ä»¥ä¸‹æƒ…å†µè¿”å› Trueï¼š
            - æˆåŠŸç”Ÿæˆ RSS æ–‡ä»¶
            - æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»ç›®å½•ï¼ˆè·³è¿‡ç”Ÿæˆï¼‰
            - æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼ˆç”Ÿæˆç©º Feedï¼‰
        ä»…åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å› Falseã€‚
    """
    rss_file = SITE_DIR / "feed.xml"
    dirs = get_feed_dirs()

    if not dirs:
        print("âš ï¸ è·³è¿‡ RSS è®¢é˜…æºç”Ÿæˆ: æœªé…ç½®ä»»ä½•ç›®å½•ã€‚")
        return True

    # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªç›®å½•å­˜åœ¨
    existing = {d for d in dirs if (SITE_DIR / d).exists()}
    missing = dirs - existing

    for d in missing:
        print(f"âš ï¸ è­¦å‘Š: é…ç½®çš„ç›®å½• '{d}' ä¸å­˜åœ¨ã€‚")

    if not existing:
        print("âš ï¸ è·³è¿‡ RSS è®¢é˜…æºç”Ÿæˆ: é…ç½®çš„ç›®å½•éƒ½ä¸å­˜åœ¨ã€‚")
        return True

    # æ”¶é›†æ–‡ç« 
    posts = collect_posts(existing, site_url)

    if not posts:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼ŒRSS è®¢é˜…æºä¸ºç©ºã€‚")
        return True

    # æŒ‰æ—¥æœŸé™åºæ’åº
    posts = sorted(posts, key=lambda x: x["date"], reverse=True)

    # è·å–é…ç½®ä¿¡æ¯
    index_html = SITE_DIR / "index.html"
    parser = parse_html_metadata(index_html)

    lang = parser["lang"]
    site_title = parser["title"].strip()
    site_description = parser.get("description", "").strip()

    config = {
        "site_url": site_url,
        "site_title": site_title,
        "site_description": site_description,
        "lang": lang,
    }

    # æ„å»º RSS XML
    try:
        rss_content = build_rss_xml(posts, config)
        rss_file.write_text(rss_content, encoding="utf-8")
        print(f"âœ… RSS è®¢é˜…æºç”ŸæˆæˆåŠŸ: {rss_file} ({len(posts)} ç¯‡æ–‡ç« )")
        return True
    except ValueError as e:
        print("âŒ é”™è¯¯: RSS è®¢é˜…æºç”Ÿæˆå¤±è´¥")
        print(f"   åŸå› : feedgen åº“æŠ¥é”™ - {e}")
        print("   è§£å†³: è¯·æ£€æŸ¥ config.typ ä¸­çš„å¿…éœ€é…ç½®å­—æ®µï¼ˆtitle å’Œ descriptionï¼‰")
        return False
    except Exception as e:
        print("âŒ é”™è¯¯: ç”Ÿæˆ RSS è®¢é˜…æºæ—¶å‡ºé”™")
        print(f"   å¼‚å¸¸: {type(e).__name__}: {e}")
        return False


def generate_sitemap(site_url: str) -> bool:
    """
    ä½¿ç”¨ Python æ ‡å‡†åº“ xml.etree.ElementTree ç”Ÿæˆ sitemap.xmlã€‚
    """
    import xml.etree.ElementTree as ET

    sitemap_path = SITE_DIR / "sitemap.xml"
    sitemap_ns = "http://www.sitemaps.org/schemas/sitemap/0.9"

    # æ³¨å†Œé»˜è®¤å‘½åç©ºé—´
    ET.register_namespace("", sitemap_ns)

    # åˆ›å»ºæ ¹å…ƒç´ 
    urlset = ET.Element("urlset", xmlns=sitemap_ns)

    # éå† _site ç›®å½•
    for file_path in sorted(SITE_DIR.rglob("*.html")):
        rel_path = file_path.relative_to(SITE_DIR).as_posix()

        # ç¡®å®š URL è·¯å¾„
        if rel_path == "index.html":
            url_path = ""
        elif rel_path.endswith("/index.html"):
            url_path = rel_path.removesuffix("index.html")
        elif rel_path.endswith(".html"):
            url_path = rel_path.removesuffix(".html") + "/"
        else:
            url_path = rel_path

        full_url = f"{site_url}/{url_path}"

        # è·å–æœ€åä¿®æ”¹æ—¶é—´
        mtime = file_path.stat().st_mtime
        lastmod = datetime.fromtimestamp(mtime).strftime("%Y-%m-%d")

        # åˆ›å»º url å…ƒç´ 
        url_elem = ET.SubElement(urlset, "url")
        ET.SubElement(url_elem, "loc").text = full_url
        ET.SubElement(url_elem, "lastmod").text = lastmod

    # ç”Ÿæˆ XML å­—ç¬¦ä¸²
    ET.indent(urlset, space="  ")
    xml_str = ET.tostring(urlset, encoding="unicode", xml_declaration=False)
    sitemap_content = f'<?xml version="1.0" encoding="UTF-8"?>\n{xml_str}'

    try:
        sitemap_path.write_text(sitemap_content, encoding="utf-8")
        print(f"âœ… Sitemap æ„å»ºå®Œæˆ: åŒ…å« {len(urlset)} ä¸ªé¡µé¢")
        return True
    except Exception as e:
        print(f"âŒ Sitemap æ„å»ºå¤±è´¥: {e}")
        return False


def generate_robots_txt(site_url: str) -> bool:
    """
    Generate robots.txt pointing to the sitemap.
    """
    robots_content = f"""User-agent: *
Allow: /

Sitemap: {site_url}/sitemap.xml
"""

    try:
        (SITE_DIR / "robots.txt").write_text(robots_content, encoding="utf-8")
        return True
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ robots.txt å¤±è´¥: {e}")
        return False


def build(force: bool = False) -> bool:
    """
    å®Œæ•´æ„å»ºï¼šHTML + PDF + èµ„æºã€‚

    å‚æ•°:
        force: æ˜¯å¦å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»¶
    """
    print("-" * 60)
    if force:
        clean()
        print("ğŸ› ï¸ å¼€å§‹å®Œæ•´æ„å»º...")
    else:
        print("ğŸš€ å¼€å§‹å¢é‡æ„å»º...")
    print("-" * 60)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    SITE_DIR.mkdir(parents=True, exist_ok=True)

    results = []

    print()
    results.append(build_html(force))
    results.append(build_pdf(force))
    print()

    results.append(copy_assets())
    results.append(copy_content_assets(force))

    if site_url := get_site_url():
        results.append(generate_sitemap(site_url))
        results.append(generate_robots_txt(site_url))
        results.append(generate_rss(site_url))

    print("-" * 60)
    if all(results):
        print("âœ… æ‰€æœ‰æ„å»ºä»»åŠ¡å®Œæˆï¼")
        print(f"  ğŸ“‚ è¾“å‡ºç›®å½•: {SITE_DIR.absolute()}")
    else:
        print("âš  æ„å»ºå®Œæˆï¼Œä½†æœ‰éƒ¨åˆ†ä»»åŠ¡å¤±è´¥ã€‚")
    print("-" * 60)

    return all(results)


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================


def create_parser() -> argparse.ArgumentParser:
    """
    åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ã€‚
    """
    parser = argparse.ArgumentParser(
        prog="build.py",
        description="Tufted Blog Template æ„å»ºè„šæœ¬ - å°† content ä¸­çš„ Typst æ–‡ä»¶ç¼–è¯‘ä¸º HTML å’Œ PDF",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ„å»ºè„šæœ¬é»˜è®¤åªé‡æ–°ç¼–è¯‘ä¿®æ”¹è¿‡çš„æ–‡ä»¶ï¼Œå¯ä½¿ç”¨ -f/--force é€‰é¡¹å¼ºåˆ¶å®Œæ•´é‡å»ºï¼š
    uv run build.py build --force
    æˆ– python build.py build -f

ä½¿ç”¨ preview å‘½ä»¤å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨ï¼š
    uv run build.py preview
    æˆ– python build.py preview -p 3000  # ä½¿ç”¨è‡ªå®šä¹‰ç«¯å£

æ›´å¤šä¿¡æ¯è¯·å‚é˜… README.md
""",
    )

    subparsers = parser.add_subparsers(dest="command", title="å¯ç”¨å‘½ä»¤", metavar="<command>")

    build_parser = subparsers.add_parser("build", help="å®Œæ•´æ„å»º (HTML + PDF + èµ„æº)")
    build_parser.add_argument("-f", "--force", action="store_true", help="å¼ºåˆ¶å®Œæ•´é‡å»º")

    html_parser = subparsers.add_parser("html", help="ä»…æ„å»º HTML æ–‡ä»¶")
    html_parser.add_argument("-f", "--force", action="store_true", help="å¼ºåˆ¶å®Œæ•´é‡å»º")

    pdf_parser = subparsers.add_parser("pdf", help="ä»…æ„å»º PDF æ–‡ä»¶")
    pdf_parser.add_argument("-f", "--force", action="store_true", help="å¼ºåˆ¶å®Œæ•´é‡å»º")

    subparsers.add_parser("assets", help="ä»…å¤åˆ¶é™æ€èµ„æº")
    subparsers.add_parser("clean", help="æ¸…ç†ç”Ÿæˆçš„æ–‡ä»¶")

    preview_parser = subparsers.add_parser("preview", help="å¯åŠ¨æœ¬åœ°é¢„è§ˆæœåŠ¡å™¨")
    preview_parser.add_argument(
        "-p", "--port", type=int, default=8000, help="æœåŠ¡å™¨ç«¯å£å·ï¼ˆé»˜è®¤: 8000ï¼‰"
    )
    preview_parser.add_argument(
        "--no-open", action="store_false", dest="open_browser", help="ä¸è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨"
    )
    preview_parser.set_defaults(open_browser=True)

    return parser


if __name__ == "__main__":
    parser = create_parser()
    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        sys.exit(0)

    # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
    script_dir = Path(__file__).parent.absolute()
    os.chdir(script_dir)

    # è·å– force å‚æ•°
    force = getattr(args, "force", False)

    # ä½¿ç”¨ match-case æ‰§è¡Œå¯¹åº”çš„å‘½ä»¤
    match args.command:
        case "build":
            success = build(force)
        case "html":
            success = build_html(force)
        case "pdf":
            success = build_pdf(force)
        case "assets":
            success = copy_assets()
        case "clean":
            success = clean()
        case "preview":
            success = preview(getattr(args, "port", 8000), getattr(args, "open_browser", True))
        case _:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {args.command}")
            success = False

    sys.exit(0 if success else 1)
