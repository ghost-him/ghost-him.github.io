#import "../../index.typ": template, tufted
#import "@preview/theorion:0.4.1": *
#show: template.with(title: "记录一下 6.5840 2025 Spring Lab3 的过程")


= 记录一下 6.5840 2025 Spring Lab3 的过程

== 实验要求

#tufted.margin-note[
  该实验的地址如下：#link("https://pdos.csail.mit.edu/6.824/labs/lab-raft1.html")[mit 6.824] \
  实验要求的部分为原文翻译并做适当的修改
]

在本实验中，需要实现 Raft，这是一种复制状态机协议。

复制服务通过在多个副本服务器上存储其状态（即数据）的完整副本来实现容错。即使某些服务器发生故障（崩溃、网络中断或不稳定），复制功能也能让服务继续运行。挑战在于，故障可能会导致不同副本之间持有的数据副本不一致。

Raft 将客户端请求组织成一个序列（称为日志），并确保所有副本服务器看到相同的日志。每个副本按日志顺序执行客户端请求，并将其应用于服务状态的本地副本。由于所有存活的副本都能看到相同的日志内容，它们都会按相同的顺序执行相同的请求，从而保持一致的服务状态。如果某个服务器故障后稍后恢复，Raft 会负责将其日志更新至最新状态。只要至少大多数（majority）服务器存活且能相互通信，Raft 就会继续运行。如果没有达到大多数，Raft 将停止进度，但一旦大多数服务器能够再次通信，它就会从中断的地方继续。

在代码中，需要遵循 Raft 扩展论文中的设计。需要实现论文中的大部分内容，包括保存持久化状态并在节点故障重启后读取它，不需要实现集群成员变更（论文中的第6节）

== 笔记内容

#quote-box[
  以下的内容是对论文 \
  #link("https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf")[In Search of an Understandable Consensus Algorithm (Extended Version)] 的翻译 \
  同时删除了适当的内容，只保留了对于 Raft 算法流程的介绍
]



=== 引言

什么是一致性算法？

一致性算法允许一组机器作为一个协调的整体工作，从而在部分成员故障时仍能正常民运行。因此它们在构建可靠的大规模软件系统中起着关键作用。

Raft的设计初衷：由于Paxos过于难学，因此作者需要找到一种新的一致性算法，同时要有高可理解性，比Paxos显著容易学习。

Raft相比与现有的一致性算法，有几个新颖点：

+ 强领导者：Raft 使用比其他一致性算法更强的领导形式。例如，日志条目仅从领导者流向其他服务器。这简化了复制日志的管理，并使 Raft 更易于理解。
+ 领导者选举：Raft 使用随机定时器来选举领导者。这在任何一致性算法已要求的“心跳”机制基础上仅增加了少量的机制，同时能简单快速地解决冲突。
+ 成员变更：Raft 变更集群服务器集的新机制使用了“联合一致性”（joint consensus）方法，在转换期间两个不同配置的“大多数”会发生重叠。这允许集群在配置更改期间继续正常运行。

=== 复制状态机
#{
  tufted.margin-note(image("asset/figure1.webp"))
}

一致性算法通常出现在需要重复状态机的背景下。如果一组拥有相同状态机的服务器出现了少部分故障，那么他们依然能继续运行。

而复制状态机通常使用复制日志的方式来实现。从图1中可以看到，每个服务器存储一个包含一系列命令的日志（log），其状态机（State Machine）按顺序执行这些命令。每个日志包含相同顺序的相同命令，因此每个状态机处理相同的命令序列。由于状态机是确定性的，因此每个状态机都会计算出相同的状态与相同的输出序列。

那么一致性算法是干什么的呢？即确保这些服务器中的日志是一样的。服务器上有一个一致性模块（Consensus Module），该模型接收来自客户端的命令并将其添加到自己的日志中。它与其他服务器上的一致性模块通信，以确保即使某些服务器发生故障，每个日志最终也包含相同顺序的相同请求。一旦命令被正确复制，每个服务器的状态机都会按日志顺序处理它们，并将输出返回给客户端。结果，这些服务器看起来就像形成了一个单一的、高度可靠的状态机。

实际系统中的一致性算法有以下的属性：
- 在所有的 non-Byzantine #footnote[ai 的翻译是: 非拜占庭条件。它的核心是故障-停止模型。节点要么正常工作，要么彻底停止。一旦运行，则严格遵守算法定义的逻辑，不会欺骗其他的服务器，也不会发送伪造的消息]条件下是安全的。
- 只要任何大多数服务器#footnote[比如5节点的集群可以容忍任意两台服务器故障]可以运行并能相互通信及与客户端通信，它们就是完全功能可用的。
- 不依赖时序来确保日志的一致性：错误的时钟与极端的延迟在最坏的情况下也只会导致可用性的问题。
- 在通常情况下，一旦集群的大多数节点对一轮RPC做出了响应，命令即可完成；少数慢速的服务器不会影响系统的整体性能。

=== Raft 算法

Raft 通过首先选举一个领导者，然后赋予该领导者管理复制日志的全权来实现共识。领导者从客户端接收日志条目，将其复制到其他服务器上，并在安全性允许时告知服务器将日志条目应用到其状态机中。拥有领导者简化了复制日志的管理。领导者可能会失效或与其他服务器迷情工连接，在这种情况下会选举产生新的领导者

Raft 将一致性问题分解为3个相对独立的子问题：
- 领导者选举：当现有的领导者失效时，必须选出一个新的领导者
- 日志复制：领导者必须从客户端接收日志条目，并将其跨集群复制，强制其他日志与其自身保持一致
- 安全性：如果任何服务器已将特定的日志条目应用到其状态机中，则其他服务器不得在相同的日志索引处应用不同的命令。

==== Raft 基础

#{
  tufted.margin-note(image("asset/figure2.webp"))
}


一个 Raft 集群有若干的服务器。在任何给定时间，每台服务器都处于三种状态之一：领导者（leader），跟随者（follower）或候选人（candidate）。在正常的运行期间，只有一个领导者，而其他的服务器都是跟随者，都是被动的，它们不会主动发生请求，而只是响应来自领导者与候选人的请求。领导者处理所有客户端请求#footnote[如果跟随者收到了客户端的请求，则会将其重定向到领导者]。候选人用于选举新的领导者。这三种状态通过右图的方式完成转换。


Raft 将时间划分为任意长度的任期（terms），如右图所示。每个任期都以一次选举开始，其中一个或多个候选人尝试按照 @leader_election 所述成为领导者。如果一个候选人赢得了选举，它将在该任期的剩余时间内担任领导者。在某些情况下，选举可能会导致选票瓜分#footnote[即：没有任何一个候选人赢得了超过半数的选票，导致该任期内无法选出领导者]。在这种情况下，该任期结束时没有领导者；新的任期（以及新的选举）将很快开始。Raft 确保在给定的任期内最多只有一个领导者。

#{
  tufted.margin-note(image("asset/figure3.webp"))
}


不同的服务器可能会在不同的时间观察到任期之间的转换，并且在某些情况下，服务器无法观察到某次选举甚至整体任期。为了解决这种问题，在 Raft 中，选择了*任期*充当了逻辑时钟，他们允许服务器检测过时的信息#footnote[比如，过时的领导者]。每台服务器都存储一个当前的任期号，该编号随时间单调递增。每当服务器通信时，都会交换当前的任期；如果一个服务器的当前任期小于另一个服务器，则它会将其当前任期更新为较大的值。如果候选人或领导都发现其任期已经过时，它会立即转回跟随者状态。如果服务器收到带有过时任期号的请求#footnote[这里是指过时的领导者的请求（复制日志与发送心跳等）以及过时候选人的请求（在选举期间拉票等）]，它会拒绝该请求。

Raft 服务器使用远程过程调用（RPC）进行通信，基本的一致性算法只需要两种类型的RPC：
+ RequestVote RPC：由候选人在选举期间发起
+ AppendEntires RPC：由领导者发起，用于复制日志条目并提供一种心跳韩式

==== 领导者选举 <leader_election>

Raft 使用心跳机制来触发领导者选举。当服务器启动时，它们最初作为跟随者。

只要服务器从领导者或候选人那里接收到了有效的RPC，它就会保持在跟随者的状态。领导者会定期向所有的跟随者发送心跳#footnote[不含日志条目的 AppendEntires RPC]，从而使其一直是领导者。如果跟随者在一段时间内#footnote[这段时间称为选举超时（election timeout）]没有收到通信，它就会假定没有可用的领导者，并开始选举以选出新的领导者。

开始选举时，跟随者会增加其当前任期并转换为候选人状态。然后它为自己投票，并向集群中的其他*每个*服务器*并行*发出RequestVote RPC。候选人会一直保持这种状态，直到以下三种情况之一发生：
+ 赢得了选举
+ 另一个服务器确立了自己作为领导者的地位
+ 一段时间过去后没有获胜者

以下是对上述三种状态的详细的描述：

如果一个候选人在同一任期内获得了集群中大多数服务器的选票，它就赢得了选举。在给定的任期内，每台服务器按照先来先服务的原则最多为一个候选人投票。*多数派规则确保了在特定任期内最多只有一个候选人可以赢得选举*。一旦候选人赢得了选举，它就成为了领导者。然后，它会向所有其他服务器发送心跳消息，以确立其权威并降频新的选举。

在等待选举期间，候选人可能会收到来自另一个声称是领导者的服务器的的 AppendEntires RPC。如果该领导者的任期至少与候选人的当前任期人一样大，则候选人承认该领导者的合法性并返回到跟随者的状态。如果 RPC 中的任期小于候选人的当前任期，则候选人拒绝该 RPC 并继续保持候选人状态。

第三种可能的结果是候选人既没有赢得也没有输掉选举：如果许多跟随者同时成为候选人，选票可能会被瓜分，导致没有任何候选人获得多数票。当发生这种情况时，每个候选人都会越野，并通过增加任期和发起另一轮 RequestVote RPC 来开始新的选举。为了防止这种情况再次重复，Raft 使用随机选举超时来减小选票瓜分的情况再次出现。

为了从一开始就防止选票瓜分，选举超时时间是从一个固定的区间（比如 150 \~ 300ms中随机选择的）。这分散了服务器的超时时间，使得在大多数情况下只有一个服务器会超时；它会在任何其他服务器超时之前赢得选举并发送心跳。同样的机制也用于处理选票瓜分。每个候选人*在选举开始时*重启其随机选举超时，并等待该超时过去后再开始下一次选举；这降低了在新选举中再次发生选票瓜分的可能性。

==== 日志复制

当领导者被选出后，它就开始为客户端请求提供服务。每个客户端请求都包含一个要由复制状态机执行的命令。领导者将该命令作为一个新的条目追加到日志中，然后并行地向其他每个服务器发出 AppendEntries RPC，以复制该条目。当该条目被安全地复制后，领导者将该条目应用到其状态机中，并将执行结果返回给客户端。如果跟随者崩溃或运行缓存，或者网络数据包丢失，领导都会无限期地重试 AppendEntires RPC（甚至在它已经响应客户端后），直到所有跟随者最终存储了所有的日志条目。#footnote[我写代码实现这部分的时候，对这个有些误解，如果是RPC错误，也就是网络错误，会让 sendAppendEntries 返回 false, 而对于其他的情况，比如日志不一致这些，它本身的 sendAppendEntires 函数是会返回 true 的，但是 reply 中的语义会返回 false，这里需要区别一下]

#{
  tufted.margin-note(image("asset/figure4.webp"))
}

日志组织形式如右图所示。每个日志条目存储一个状态机命令，以及领导者接收到该条目时的任期号。日志条目中的任期号用于检测日志之间的不一致。每个日志条目还有一个整数索引，用于标识其在日志中的位置。

领导者决定何时将日志条目应用到状态机是安全的。应用到状态机后的条目被称为已提交的（committed）。Raft 保证已提交的条目是持久的，并且最终会被所有可用的状态机执行。一旦创建该条目的领导者将其复制到大多数服务器上，则该日志条目就可以被视为是已提交。这同时也提交了领导者日志中所有等效的条目，包括由前任领导者创建的条目。节介绍了领导者变更后应用此规则时的一些注意点，同时 @security 证明了这种提前定义是安全的。领导者会跟踪它所知道的*已提交的*最高索引，并在未来的 AppendEntries RPC（包含心跳）中包含该索引，以便其他服务器最终能发现。一旦跟随者得知某个日志条目已提交它就会按日志顺序将该条目应用到其本地状态机中。

Raft维护以下的属性，这些属性共同构成了日志匹配属性
+ 如果不同日志中的两个条目具有相同的索引与任期，则它们存储相同的命令
+ 如果不同日志中的两个条目具有相同的索引与任期，则日志中所有先前的条目也完全相同

第一个属性：领导者在给定的任期内最多创建一个具有给定日志索引的条目，且日志条目永远不会改变其在日志中的位置。#footnote[注意，这句话的理解是：对于(term, index)的这个组合来说，全局只会存在一个，并且之后的所有操作中其*位置*不会再被修改了。因此，如果两个日志包含具有相同索引与任期的条目，则它们存储相同的命令。*不是*指领导者在其任期内只会创建一个日志条目]
第二个属性：通过 AppendEntires 执行的简单一致性检查来保证。发送 AppendEntires RPC 时，领导者会包含其日志中紧接在新条目之前的条目的索引和任期。如果跟随者在其日志中找不到具有相同索引和任务的条目，它就会拒绝新的条目。一致性检查起到了归纳步的作用：日志的初始空状态满足日志匹配属性，且每当日志扩展时，一致性检查都会保持日志匹配属性。因此，每当 AppendEntires 成功返回时，领导者就知道跟随者的日志直到新条目为止都与自身的日志相同。

正常运行时，领导者和跟随者的日志保持一致，因此 AppendEntires的一致性检查永远不会失败。然而，领导者崩溃可能会使日志变得不一致（比如旧领导者可能尚未完全复制其日志中的所有条目）。这些不一致可能会在一系列领导者和跟随者崩溃中累积。

#{
  tufted.margin-note(image("asset/figure5.webp"))
}

如右图所示，跟随者的日志可能与新领导者的日志不同的几种方式。跟随者可能会丢失领导者上存在的条目，它可能会拥有领导者上不存在的额外条目，或者两者兼而有之。日志中缺失和多余的条目可能会跨越多个任期。

在 Raft 中，领导者通过强制让跟随者复制自己的日志来处理不一致。这意味着跟随者日志中冲突条目将被领导者日志中的条目覆盖。@security_election 节将介绍这种操作结合另一项限制时，是安全的。

为了让跟随者的日志与自己的日志保持一致，领导者需要找到两个日志达到一致的最新日志条目，删除跟随者日志中该点之后的所有条目，并将该点之后的所有领导者条目发送给跟随者。这些操作都是为了响应 AppendEntries RPC 执行的一致性检查而发生的。领导者为每个跟随者维护一个 `nextIndex`，即领导者将发送给该跟随者的下一个日志条目的索引。当领导者刚掌权时，它会将所有 `nextIndex` 值初始化为其日志中最后一个索引之后的索引（即 figure 7 中的 11）。如果跟随者的日志与领导者的不一致，AppendEntires 一致性检查将在下一次 RPC 中失败。在被拒绝后，领导者会减小 `nextIndex` 并重试 AppendEntires，直到 `nextIndex` 达到领导者与跟随者日志的匹配的点。在这种情况下，AppendEntires将成功，从而删除跟随者日志中任何冲突条目，并追加领导者日志中的条目（如果存在）。一旦 AppendEntires 成功，跟随者的日志就与领导者的保持了一致，并在该任期的剩余时间下保持这种状态。

论文在这里提到了一种优化的方案#footnote[在拒绝 AppendEntries 请求时，跟随者可以包含冲突条目的任期以及它为该任期存储的第一个索引。利用这些信息，领导者可以减小 nextIndex 以避开该任期内的所有冲突条目；这样每个包含冲突条目的任期只需要一个 AppendEntries RPC，而不是每个条目一个 RPC。]，从而减少被拒绝的 AppendEntries RPC 的数量。但是论文也指出：在实践中，这种故障发生频率较低，而且不太可能出现许多不一致的条目。

通过这种机制，领导者在掌权时不需要采取任何特殊行动就可以恢复日志一致性。它只需要开始正常运行，日志就会在响应 AppendEntires 一致性检查失败时自动趋于一致。领导者永远不会覆盖或删除自己日志中的条目，这也就是*领导者的只增属性（Leader APpend-Only Property）*

这种日志复制机制展现了理想的共识属性：只要集群中大多数服务器正常运行，Raft 就可以接收、复制并应用新的日志条目；在正常情况下，新条目可以通过一轮发往集群多数派的 RPC 进行复制；且单个缓慢的跟随者不会影响整体性能。

==== 安全性 <security>

前面的章节描述了 Raft 如何选举领导者和复制日志条目。然而，到目前谈恋爱止描述机制还不足以确保每个状态机以相同的顺序执行完全相同的命令#footnote[例如，当领导者提交多个日志条目时，某个跟随者可能不可用，随后它可能被选为领导者并用新条目覆盖这些条目；结果，不同的状态机可能会执行不同的命令序列。]。

因此，本节需要对想要选举成为领导者的服务器添加一项限制。该限制可以确保任何给定任期的领导者都包含之前任期中所有已提交的条目*（领导者完备性属性 Leader Completeness Property）*。

==== 安全性-选举的限制 <security_election>

在任何基于领导者的一致性算法中，领导者最终都必须存储所有已提交的日志条目。Raft 使用了一种更简单的方法，来保证新领导者当选的时候，之前所有已提交的条目都存在于该领导者上。因此*日志条目只从领导者流向跟随者，且领导者从不覆盖其日志中已有的条目*。

Raft 使用投票过程来防止候选人赢得选举，除非其日志包含所有已提交的条目。候选人必须联系集群的多数派才能当选，这意味着每个已提交的条目必须至少存在于这些服务器中的一个。如果候选人的日志至少与该多数派中任何其他日志一样“新”，那么它将持有所有已提交的条目。而RequestVote RPC实现了这一限制：RPC 包含有关候选人日志的信息，如果投票者自己的日志比候选人的日志更“新”，则投票者会拒绝投票。

Raft 通过比较日志中最后一个条目的索引和任期来确定两个日志中哪个更“新”。如果日志的最后条目具有不同的任期，则任期号较大的日志更“新”。如果日志以相同的任期结束，则较长的日志更“新”。

==== 安全性-提交之前任期的条目

#{
  tufted.margin-note(image("asset/figure6.webp"))
}

领导者一旦发现当前任期的某个条目存储在大多数服务器上，就会认为该条目已提交。如果领导者在提交某个条目之前崩溃，未来的领导者将尝试完成该条目的复制。然而，领导者不能在之前任期的条目存储在大多数服务器上时，就立即得到该条目已提交的结论。因为可能会出现右图的情况。此时旧的日志条目存储在了大多数服务器上，但是依然可能会被未来的领导者覆盖。

为了消除图示问题，Raft 永远不会通过计算副本数来提交*之前任期的*日志条目。只有领导者*当前任期的*日志条目通过计算副本数来提交；一旦当前任期的条目以这种方式被提交，那么由于日志匹配属性，所有靠前的条目也会被间接提交。

Raft 通过引入任期号来解决这个问题，但这也增加了额外的复杂性。但是相比于其他的一致性算法，Raft对于日志条目的推理更加容易，因为日志条目在不同服务器间复制时始终保持其原始的任期号，不会被重新编号。

这种机制之所以有效，是因为选举限制确保了：拥有最新提交条目的节点，才能在未来的选举中获得多数票。对于那些没有得到最新的提交的条目的节点，它无法得到多数选票#footnote[什么是提交的？即：大多数节点都接收到的条目，而成为候选人要求得到大多数的选票。而如果跟随者发现候选人的条目不是最新的，则不会同意其成为领导者]。

==== 安全性-安全性认证

这里就不详细写了，最后的结论是：
+ 所有任期大于T的领导者必须包含任期T中所有已提交的条目
+ 日志匹配属性保证了未来的领导者也将包含被间接提交的条目

==== 跟随者和候选人崩溃

在此之前，一直讨论的是领导者的故障。而跟随者和候选人的处理比领导者要简单得多，且处理方式相同。如果跟随者或候选人，那么未来发给它的 RequestVote 和 AppendEntires RPC 将失败。Raft 通过无限期重试来处理这些失败；如果崩溃的服务器重启，RPC将成功完成。如果服务器在完成RPC后但在响应前崩溃，它在重启后将再次收到相同的RPC。Raft RPC是幂等的#footnote[比如如果跟随者收到了一个 AppendEntries 请求，其中包含已存在于其日志中的日志条目，它会忽略新请求中的这些条目]，因此这样不会出现问题。

==== 时间与可用性

Raft 的特点是安全性不可以依赖于时间：系统绝不能仅仅因为某些事件发生的比预期快或慢就产生错误的结果。但是可用性#footnote[系统及时响应客户端的能力]必须取决于时间。如果消息交换花费的时间筐服务器崩溃之间的典型间隔时间，候选人将无法保持足够长的在线时间来离得选举；没有稳定的领导者，Raft就无法取得进展。

所以领导者选举是Raft中对时间要求最关键的方面。只要系统满足以下的时间要求，Raft就可以选举并维持稳定的领导者

$ "broadcastTime" lt.double "electionTimeout" lt.double "MTBF" $

- broadcastTime 是服务器并行向集群中每台服务器发送RPC并接收响应所需的平均时间
- electionTimeout 是选举超时时间
- MTBF 是单台服务器两次故障之间的平均时间

同时，广播时间应该比选举超时小一个数量级，同时选举超时应该比MTBF小几个数量级。广播时间与MTBF是底层系统的属性，而选举超时是我们的必须选择的。一般来说，广播时间在0.5ms到20ms之间，所以选举超时选择10ms到500ms之间就行。典型的服务器MTBF是几个月或更长，这很容易满足时间的要求。

==== 总体的流程

以上就是Raft算法的所有的内容，以下是论文中的图2，也就是总览性的图

#image("asset/figure7.webp")




== 我的实现

=== 任务A

读起来还是很容易的，但是实现起代码来还是会有很多的困难的，可以发现我之前的理解还是不够的深。

比如这个选举超时时间，原来就是心跳的超时时间，这个是随机的。我一开始以为心跳超时时间是固定的，然后选举超时的时间是随机的。所以当时还想到在正式选举前，先使用随机的Sleep来来防止选举瓜分的情况。后来pass测试用例后，问了一下ai，才知道==。

经过我的优化，效果还是蛮明显的（而且还比lab中给出的示例的效果更好😎）：

这个是我一开始的实现的性能：

```bash
Test (3A): initial election (reliable network)...
  ... Passed --  time  3.5s #peers 3 #RPCs    60 #Ops    0
Test (3A): election after network failure (reliable network)...
  ... Passed --  time  5.5s #peers 3 #RPCs   138 #Ops    0
Test (3A): multiple elections (reliable network)...
  ... Passed --  time  5.5s #peers 50 #RPCs  3430 #Ops    0
PASS
ok      6.5840/raft1    14.571s
```

这个是优化后的：

```bash
Test (3A): initial election (reliable network)...
  ... Passed --  time  3.1s #peers 3 #RPCs    46 #Ops    0
Test (3A): election after network failure (reliable network)...
  ... Passed --  time  4.5s #peers 3 #RPCs    96 #Ops    0
Test (3A): multiple elections (reliable network)...
  ... Passed --  time  5.3s #peers 50 #RPCs  3038 #Ops    0
PASS
ok      6.5840/raft1    12.939s
```

不论是时间，还是总的RPC的使用数量都有比较明显的下降。

现在还没写完整个实验，剩下的等我写完实验以后再写

=== 任务B

经过我3天的奋战，终于把任务B写出来了。这个任务看起来很简单，逻辑看起来也很简单，但是花了我蛮长时间的。我感觉一个最大的问题就是有些细节不太能直接想出来，得一直看运行的时序图，找到代码中的小的逻辑bug，还有并发的bug#footnote[还是rust在并发方面的心智负担比较低，rust在编译期就能杜绝各种数据竞争什么的，心智负担特别低。写这个go又让我想起来之前写cpp时的痛苦经历QAQ]。所以我期间也在一直看论文，看笔记，最后实在有些不确定的部分，我就使用ai来分析了一下。

这个任务B主要的内容就是实现日志的复制。日志的复现主要有两个个任务：
+ 实现选举限制，即：简单一致性 @security_election
+ 根据日志复制中的相关的逻辑，完善两个RPC的发送与处理逻辑
+ 实现Start函数。需要注意这个函数的特性：需要立即返回的，而不是像论文中一样要阻塞等到日志条目被提交才返回结果。我一开始没注意到这个问题，导致卡了很久QAQ

这里还有一个需要注意的：当服务器判断出来可以提交log时，需要通过 applyCh 导出。而这个 applyCh 在Make中传入了，但是默认没有被添加到结构体中！！！这个也导致我卡了很久，我让ai给我思路的时候才发现有这个东西。我说我的思路和代码基础都对的，但是为什么我的测试一个都没通过(╯‵□′)╯︵┻━┻

经过3天的折磨，最后也是把这个Lab3B写出来了，hard果然非常的hard /\_\\

最后这个是我的运行结果：

```bash
ghost-him@lab ~/6/s/raft1 (master) [1]> go test -run 3B -race
Test (3B): basic agreement (reliable network)...
  ... Passed --  time  0.9s #peers 3 #RPCs    14 #Ops    0
Test (3B): RPC byte count (reliable network)...
  ... Passed --  time  2.0s #peers 3 #RPCs    46 #Ops    0
Test (3B): test progressive failure of followers (reliable network)...
  ... Passed --  time  4.9s #peers 3 #RPCs   108 #Ops    0
Test (3B): test failure of leaders (reliable network)...
  ... Passed --  time  6.8s #peers 3 #RPCs   214 #Ops    0
Test (3B): agreement after follower reconnects (reliable network)...
  ... Passed --  time  4.7s #peers 3 #RPCs    93 #Ops    0
Test (3B): no agreement if too many followers disconnect (reliable network)...
  ... Passed --  time  4.1s #peers 5 #RPCs   172 #Ops    0
Test (3B): concurrent Start()s (reliable network)...
  ... Passed --  time  0.5s #peers 3 #RPCs    14 #Ops    0
Test (3B): rejoin of partitioned leader (reliable network)...
  ... Passed --  time  4.9s #peers 3 #RPCs   148 #Ops    0
Test (3B): leader backs up quickly over incorrect follower logs (reliable network)...
  ... Passed --  time 16.1s #peers 5 #RPCs  2952 #Ops    0
Test (3B): RPC counts aren't too high (reliable network)...
  ... Passed --  time  2.0s #peers 3 #RPCs    52 #Ops    0
PASS
ok      6.5840/raft1    47.983s
```

除了 Test (3B): leader backs up quickly over incorrect follower logs (reliable network) 这个测试花的时间比较长，其他的速度和RPC数都比官方好一些。这里其实是可以用论文中提到的优化方法的，但是本着代码和人有一个能跑就行的原则，这里就先不做优化了😋